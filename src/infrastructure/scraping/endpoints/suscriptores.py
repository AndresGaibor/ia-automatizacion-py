from playwright.sync_api import Page
from typing import List, Tuple, Optional
from datetime import datetime

from ..models.suscriptores import (
    SubscriberScrapingData,
    SubscriberTableData,
    SubscriberFilterResult,
    CampaignSubscriberReport,
    ScrapingSession,
    SubscriberExtractionConfig
)
from src.infrastructure.api.models.campanias import CampaignBasicInfo
from src.shared.utils.legacy_utils import obtener_total_paginas, navegar_siguiente_pagina, load_config
from src.shared.logging.logger import get_logger

class SubscribersScraper:
    """Scraper para extraer datos de suscriptores de campa√±as"""

    def __init__(self):
        self.logger = get_logger()
        self.config = load_config()

    def seleccionar_filtro(self, page: Page, label: str) -> bool:
        """
        Selecciona un filtro en el selector de la p√°gina actual.
        Devuelve True si se seleccion√≥ correctamente, False en caso contrario.
        """
        try:
            select_filtro = page.locator("#query-filter")
            select_filtro.wait_for(timeout=10000)
            select_filtro.select_option(label=label)
            # Optimizaci√≥n: usar domcontentloaded en lugar de networkidle para conexiones lentas
            page.wait_for_load_state("domcontentloaded", timeout=15000)
            # Esperar un poco m√°s para que la tabla se actualice
            page.wait_for_timeout(2000)
            return True
        except Exception as e:
            self.logger.error(f"Error seleccionando filtro '{label}': {e}")
            return False

    def extraer_suscriptores_tabla(self, page: Page, cantidad_campos: int) -> List[SubscriberTableData]:
        """Extrae suscriptores de la tabla de la interfaz web"""
        suscriptores = []

        # Esperar que la tabla est√© visible antes de procesar
        page.wait_for_load_state("domcontentloaded", timeout=20000)
        page.wait_for_timeout(3000)  # Espera adicional para cargar contenido din√°mico

        try:
            self.logger.info("üîç Buscando tabla de suscriptores usando selector espec√≠fico...")
            self.logger.info(f"üìç URL actual: {page.url}")

            # Usar el selector que funciona seg√∫n el c√≥digo original
            tabla_suscriptores = page.locator('ul').filter(has=page.locator("li", has_text="Correo electr√≥nico"))
            self.logger.info(f"üîç Tabla suscriptores encontrada: {tabla_suscriptores.count()} elementos")

            suscriptores_elementos = tabla_suscriptores.locator('> li')
            cantidad_suscriptores = suscriptores_elementos.count()

            self.logger.info(f"üìÑ {cantidad_suscriptores} elementos encontrados en la tabla")

            if cantidad_suscriptores == 0:
                self.logger.warning("‚ö†Ô∏è No se encontraron elementos en la tabla")
                # Intentar detectar qu√© hay en la p√°gina
                self.logger.info("üîç Intentando encontrar cualquier lista ul...")
                listas_ul = page.locator('ul')
                self.logger.info(f"üìã {listas_ul.count()} listas ul encontradas en total")

                for i in range(min(5, listas_ul.count())):  # Revisar las primeras 5 listas
                    try:
                        lista_text = listas_ul.nth(i).inner_text()[:100]  # Primeros 100 caracteres
                        self.logger.info(f"üìã Lista {i}: {lista_text}")
                    except:
                        self.logger.info(f"üìã Lista {i}: No se pudo leer")
                return suscriptores

            # Saltar el primer elemento (encabezados) y procesar el resto
            for i in range(1, cantidad_suscriptores):  # empieza en 1 ‚Üí segundo elemento
                try:
                    datos_suscriptor = suscriptores_elementos.nth(i).locator('> div')
                    self.logger.info(f"üîç Procesando elemento {i}, divs encontrados: {datos_suscriptor.count()}")

                    # Extraer exactamente 4 campos como en el c√≥digo original
                    correo = datos_suscriptor.nth(0).inner_text().strip() if datos_suscriptor.count() > 0 else ""
                    lista = datos_suscriptor.nth(1).inner_text().strip() if datos_suscriptor.count() > 1 else ""
                    estado = datos_suscriptor.nth(2).inner_text().strip() if datos_suscriptor.count() > 2 else ""
                    calidad = datos_suscriptor.nth(3).inner_text().strip() if datos_suscriptor.count() > 3 else ""

                    self.logger.info(f"‚úÖ Elemento {i}: correo={correo}, lista={lista}, estado={estado}, calidad={calidad}")

                    # Solo a√±adir si hay datos v√°lidos (al menos correo)
                    if correo:
                        suscriptores.append(SubscriberTableData(
                            correo=correo,
                            lista=lista,
                            estado=estado,
                            calidad=calidad
                        ))

                except Exception as e:
                    self.logger.error(f"‚ùå Error procesando elemento {i}: {e}")
                    continue

        except Exception as e:
            self.logger.error(f"‚ùå Error cr√≠tico extrayendo suscriptores de tabla: {e}")

        self.logger.info(f"üìà {len(suscriptores)} suscriptores extra√≠dos de la tabla")
        return suscriptores

    def navegar_a_detalle_suscriptores(self, page: Page, campaign_id: int) -> bool:
        """
        Navega a la secci√≥n de detalles de suscriptores de la campa√±a.
        Devuelve True si se naveg√≥ correctamente, False en caso contrario.
        """
        try:
            url_base = self.config.get("url_base", "")
            url = f"{url_base}/report/campaign/{campaign_id}/"
            self.logger.info(f"üó∫Ô∏è Navegando a: {url}")
            page.goto(url, timeout=30000)

            self.logger.info("üîó Buscando enlace 'Detalles suscriptores'")
            detalles_link = page.get_by_role("link", name="Detalles suscriptores")
            detalles_link.wait_for(timeout=15000)
            detalles_link.click()
            self.logger.info("‚úÖ Clic en 'Detalles suscriptores' exitoso")

            page.wait_for_load_state("domcontentloaded", timeout=20000)
            page.wait_for_timeout(2000)  # Espera adicional para cargar contenido
            self.logger.info("‚úÖ P√°gina de detalles de suscriptores cargada")
            return True
        except Exception as e:
            self.logger.error(f"‚ùå Error navegando a Detalles suscriptores: {e}")
            return False

    def extraer_datos_filtro(self, page: Page, campania: CampaignBasicInfo, filter_type: str) -> SubscriberFilterResult:
        """
        Funci√≥n auxiliar para extraer datos de un filtro espec√≠fico.
        """
        suscriptores_resultado = []

        try:
            # Usar funci√≥n r√°pida primero, luego optimizada si hay muchas p√°ginas
            self.logger.info("üìÅ Detectando n√∫mero de p√°ginas...")
            paginas_totales = obtener_total_paginas(page)
            if paginas_totales > 3:  # Si hay m√°s de 3 p√°ginas, optimizar items por p√°gina
                self.logger.info(f"üöÄ Detectadas {paginas_totales} p√°ginas, optimizando items por p√°gina...")
                paginas_totales = obtener_total_paginas(page)
            self.logger.info(f"üìÑ Procesando {paginas_totales} p√°ginas en total")

            for numero_pagina in range(1, paginas_totales + 1):
                self.logger.info(f"üìÉ Procesando p√°gina {numero_pagina}/{paginas_totales}")
                try:
                    # Extraer suscriptores de la p√°gina actual
                    suscriptores_pagina = self.extraer_suscriptores_tabla(page, 4)
                    self.logger.info(f"‚úÖ P√°gina {numero_pagina}: {len(suscriptores_pagina)} suscriptores extra√≠dos")

                    # Procesar cada suscriptor de esta p√°gina
                    for suscriptor in suscriptores_pagina:
                        suscriptores_resultado.append(SubscriberScrapingData(
                            proyecto=campania.name or "",
                            lista=suscriptor.lista,
                            correo=suscriptor.correo,
                            lista2=suscriptor.lista,
                            estado=suscriptor.estado,
                            calidad=suscriptor.calidad
                        ))
                except Exception as e:
                    self.logger.error(f"‚ùå Error procesando p√°gina {numero_pagina}: {e}")
                    continue

                # Navegar a la siguiente p√°gina si no es la √∫ltima
                if numero_pagina < paginas_totales:
                    self.logger.info(f"‚û°Ô∏è Navegando a p√°gina {numero_pagina + 1}")
                    if not navegar_siguiente_pagina(page, numero_pagina):
                        self.logger.error(f"‚ùå No se pudo navegar a la p√°gina {numero_pagina + 1}")
                        break

        except Exception as e:
            self.logger.error(f"‚ùå Error cr√≠tico extrayendo datos del filtro: {e}")

        self.logger.info(f"üìà Total suscriptores extra√≠dos: {len(suscriptores_resultado)}")

        return SubscriberFilterResult(
            filter_type=filter_type,
            subscribers=suscriptores_resultado,
            total_pages=paginas_totales,
            total_subscribers=len(suscriptores_resultado)
        )

    def extraer_hard_bounces(self, page: Page, campania: CampaignBasicInfo, campaign_id: int) -> List[SubscriberScrapingData]:
        """
        Scrapea Hard bounces de la campa√±a y devuelve filas.
        """
        suscriptores_resultado = []
        try:
            self.navegar_a_detalle_suscriptores(page, campaign_id)
            self.seleccionar_filtro(page, "Hard bounces")

            # Optimizaci√≥n: Obtener el m√°ximo de elementos por p√°gina primero
            paginas_totales = obtener_total_paginas(page)

            for numero_pagina in range(1, paginas_totales + 1):
                # Extraer suscriptores de la p√°gina actual
                suscriptores_pagina = self.extraer_suscriptores_tabla(page, 4)

                # Procesar cada suscriptor de esta p√°gina
                for suscriptor in suscriptores_pagina:
                    suscriptores_resultado.append(SubscriberScrapingData(
                        proyecto=campania.name or "",
                        lista=suscriptor.lista,
                        correo=suscriptor.correo,
                        lista2=suscriptor.lista,
                        estado=suscriptor.estado,
                        calidad=suscriptor.calidad
                    ))

                # Navegar a la siguiente p√°gina si no es la √∫ltima
                if numero_pagina < paginas_totales:
                    if not navegar_siguiente_pagina(page, numero_pagina):
                        self.logger.error(f"No se pudo navegar a la p√°gina {numero_pagina + 1}")
                        break
        except Exception as e:
            self.logger.error(f"Error generando Hard bounces para campa√±a {campaign_id}: {e}")

        return suscriptores_resultado

    def extraer_no_abiertos(self, page: Page, campania: CampaignBasicInfo, campaign_id: int) -> List[SubscriberScrapingData]:
        """
        Scrapea No abiertos en la p√°gina de detalles abierta y devuelve filas.
        Nota: Asume que ya estamos en la vista de 'Detalles suscriptores' de la campa√±a.
        """
        suscriptores_resultado = []
        try:
            self.seleccionar_filtro(page, "No abiertos")

            # La funci√≥n obtener_total_paginas ya optimiza elementos por p√°gina
            paginas_totales = obtener_total_paginas(page)

            for numero_pagina in range(1, paginas_totales + 1):
                # Extraer suscriptores de la p√°gina actual
                suscriptores_pagina = self.extraer_suscriptores_tabla(page, 4)

                # Procesar cada suscriptor de esta p√°gina
                for suscriptor in suscriptores_pagina:
                    suscriptores_resultado.append(SubscriberScrapingData(
                        proyecto=campania.name or "",
                        lista=suscriptor.lista,
                        correo=suscriptor.correo,
                        lista2=suscriptor.lista,
                        estado=suscriptor.estado,
                        calidad=suscriptor.calidad
                    ))

                # Navegar a la siguiente p√°gina si no es la √∫ltima
                if numero_pagina < paginas_totales:
                    if not navegar_siguiente_pagina(page, numero_pagina):
                        self.logger.error(f"No se pudo navegar a la p√°gina {numero_pagina + 1}")
                        break
        except Exception as e:
            self.logger.error(f"Error generando No abiertos para campa√±a {campaign_id}: {e}")

        return suscriptores_resultado

    def extraer_suscriptores_optimizado(self, page: Page, campania: CampaignBasicInfo, campaign_id: int) -> Tuple[List[SubscriberScrapingData], List[SubscriberScrapingData]]:
        """
        Optimizaci√≥n: Extrae tanto hard bounces como no abiertos en una sola navegaci√≥n
        para reducir el tiempo total de ejecuci√≥n.
        Devuelve: (hard_bounces, no_abiertos)
        """
        hard_bounces = []
        no_abiertos = []

        try:
            # Navegar una sola vez a la secci√≥n de detalles
            self.logger.info(f"üó∫Ô∏è Navegando a detalles de suscriptores para campa√±a {campaign_id}")
            if not self.navegar_a_detalle_suscriptores(page, campaign_id):
                self.logger.error(f"‚ùå No se pudo navegar a detalles para campa√±a {campaign_id}")
                return hard_bounces, no_abiertos

            self.logger.info("‚úÖ Navegaci√≥n exitosa a detalles de suscriptores")

            # Extraer Hard bounces primero
            self.logger.info(f"üì¶ Extrayendo Hard bounces para campa√±a {campaign_id}")
            try:
                if self.seleccionar_filtro(page, "Hard bounces"):
                    result = self.extraer_datos_filtro(page, campania, "Hard bounces")
                    hard_bounces = result.subscribers
                    self.logger.info(f"‚úÖ {len(hard_bounces)} hard bounces extra√≠dos")
                else:
                    self.logger.warning("‚ö†Ô∏è No se pudo seleccionar filtro Hard bounces")
            except Exception as e:
                self.logger.error(f"‚ùå Error extrayendo hard bounces: {e}")

            # Extraer No abiertos despu√©s
            self.logger.info(f"üìß Extrayendo No abiertos para campa√±a {campaign_id}")
            try:
                if self.seleccionar_filtro(page, "No abiertos"):
                    result = self.extraer_datos_filtro(page, campania, "No abiertos")
                    no_abiertos = result.subscribers
                    self.logger.info(f"‚úÖ {len(no_abiertos)} no abiertos extra√≠dos")
                else:
                    self.logger.warning("‚ö†Ô∏è No se pudo seleccionar filtro No abiertos")
            except Exception as e:
                self.logger.error(f"‚ùå Error extrayendo no abiertos: {e}")

        except Exception as e:
            self.logger.error(f"‚ùå Error cr√≠tico en extracci√≥n optimizada para campa√±a {campaign_id}: {e}")

        self.logger.info(f"üìà Resumen extracci√≥n campa√±a {campaign_id}: {len(hard_bounces)} hard bounces, {len(no_abiertos)} no abiertos")
        return hard_bounces, no_abiertos

    def extraer_suscriptores_completos(
        self,
        page: Page,
        campania: CampaignBasicInfo,
        campaign_id: int,
        config: Optional[SubscriberExtractionConfig] = None
    ) -> CampaignSubscriberReport:
        """
        Extrae todos los tipos de suscriptores de una campa√±a usando configuraci√≥n.
        """
        if config is None:
            config = SubscriberExtractionConfig()

        session = ScrapingSession(
            session_id=f"campaign_{campaign_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            campaign_ids=[campaign_id]
        )

        report = CampaignSubscriberReport(
            campaign_id=campaign_id,
            campaign_name=campania.name or "",
            fecha_envio=campania.date_sent or ""
        )

        start_time = datetime.now()

        try:
            # Extraer hard bounces y no abiertos usando el m√©todo optimizado
            if config.use_optimized_extraction:
                hard_bounces, no_abiertos = self.extraer_suscriptores_optimizado(page, campania, campaign_id)
                if config.extract_hard_bounces:
                    report.hard_bounces = hard_bounces
                if config.extract_no_abiertos:
                    report.no_abiertos = no_abiertos
            else:
                # Extraer individualmente
                if config.extract_hard_bounces:
                    report.hard_bounces = self.extraer_hard_bounces(page, campania, campaign_id)
                if config.extract_no_abiertos:
                    report.no_abiertos = self.extraer_no_abiertos(page, campania, campaign_id)

            # Los abiertos, clics y soft bounces normalmente se obtienen por API
            # pero si se requiere scraping se puede implementar aqu√≠

            session.total_subscribers_extracted = report.total_subscribers
            session.complete_session()

        except Exception as e:
            session.add_error(str(e))
            session.complete_session()
            raise

        end_time = datetime.now()
        report.processing_time_seconds = (end_time - start_time).total_seconds()

        return report