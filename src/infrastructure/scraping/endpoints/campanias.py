"""
Scraper para endpoints de campa√±as que NO existen en la API de Acumbamail

IMPORTANTE: Este archivo contiene m√©todos ESQUELETO que necesitas implementar.
Cada m√©todo tiene comentarios TODO indicando qu√© necesitas hacer.
"""
from typing import List, Optional, Dict
from playwright.sync_api import Page
import time
from datetime import datetime

from ..base import BaseScraper, ScrapingConfig
from ..models.campanias import (
    ScrapedNonOpener,
    ScrapedHardBounce,
    ScrapedCampaignStats,
    ScrapedGeographicStats,
    ScrapedDeviceStats,
    ScrapedCampaignData,
    ScrapedCampaignUrl
)
from ..utils.selectors import CampaignSelectors, CommonSelectors
from ..utils.navigation import NavigationHelper
from src.shared.logging.logger import get_logger

logger = get_logger()


class CampaignsScraper(BaseScraper):
    """
    Scraper para endpoints de campa√±as que no existen en API
    
    Proporciona acceso a datos como:
    - Suscriptores que NO abrieron emails
    - Hard bounces detallados  
    - Estad√≠sticas geogr√°ficas y por dispositivo
    """
    
    def __init__(self, page: Page, config: Optional[ScrapingConfig] = None):
        super().__init__(page, config)
        self.selectors = CampaignSelectors()
        self.common = CommonSelectors()
        self.navigation = NavigationHelper(page)
    
    # === M√âTODO PRINCIPAL: NO-OPENERS ===
    def get_non_openers(self, campaign_id: int) -> List[ScrapedNonOpener]:
        """
        ‚≠ê M√âTODO PRIORITARIO: Obtener suscriptores que NO abrieron la campa√±a
        
        Este es el m√©todo m√°s importante ya que esta informaci√≥n no est√° 
        disponible en la API de Acumbamail.
        
        Args:
            campaign_id: ID de la campa√±a
            
        Returns:
            Lista de suscriptores que no abrieron el email
            
        TODO: IMPLEMENTAR ESTE M√âTODO PRIMERO
        """
        logger.info(f"üîç Iniciando scraping de no-openers para campa√±a {campaign_id}")
        start_time = time.time()
        
        def _scrape_non_openers():
            # TODO: 1. Navegar a la p√°gina de la campa√±a
            self.navigate_to_campaign(campaign_id)
            
            # TODO: 2. Ir a la secci√≥n correcta (estad√≠sticas/suscriptores)
            # self.navigation.go_to_campaign_tab(campaign_id, "statistics")
            
            # TODO: 3. Buscar la secci√≥n de "No abrieron" / "Unopened"
            # self.wait_for_element(self.selectors.non_opener_emails)
            
            # TODO: 4. Extraer emails con paginaci√≥n
            all_non_openers = []
            
            def extract_emails_from_page(elements):
                """
                TODO: Implementar extracci√≥n de emails de la p√°gina actual
                
                Args:
                    elements: Lista de elementos HTML encontrados
                    
                Returns:
                    Lista de objetos ScrapedNonOpener
                """
                page_emails = []
                
                # TODO: Iterar por cada elemento y extraer:
                # - Email del suscriptor
                # - Fecha de env√≠o (si est√° disponible)  
                # - Nombre del suscriptor (si est√° disponible)
                # - Lista de origen (si est√° disponible)
                
                for element in elements:
                    try:
                        # TODO: Extraer email del elemento
                        # email = element.text_content().strip()
                        # O usar selector m√°s espec√≠fico si es una tabla:
                        # email = element.query_selector(".email-cell").text_content()
                        
                        # TODO: Crear objeto ScrapedNonOpener
                        # non_opener = ScrapedNonOpener(
                        #     email=email,
                        #     campaign_id=campaign_id,
                        #     date_sent=self._extract_date_sent(element),
                        #     subscriber_name=self._extract_subscriber_name(element),
                        #     list_name=self._extract_list_name(element)
                        # )
                        # page_emails.append(non_opener)
                        
                        # PLACEHOLDER: Remover cuando implementes
                        pass
                        
                    except Exception as e:
                        logger.warning(f"Error extrayendo email de elemento: {e}")
                        continue
                
                return page_emails
            
            # TODO: 5. Usar paginaci√≥n autom√°tica
            # all_non_openers = self.handle_pagination(
            #     item_selector=self.selectors.non_opener_emails,
            #     next_button_selector=self.selectors.next_page_button,
            #     extract_func=extract_emails_from_page
            # )
            
            # PLACEHOLDER: Remover cuando implementes
            logger.warning("‚ö†Ô∏è  get_non_openers() NO IMPLEMENTADO - retornando lista vac√≠a")
            
            duration = time.time() - start_time
            logger.info(f"‚úÖ No-openers scraping completado: {len(all_non_openers)} emails en {duration:.1f}s")
            return all_non_openers
        
        return self.wait_and_retry(_scrape_non_openers)
    
    # === M√âTODO SECUNDARIO: HARD BOUNCES ===
    def get_hard_bounces(self, campaign_id: int) -> List[ScrapedHardBounce]:
        """
        üîß Obtener hard bounces detallados de la campa√±a
        
        Aunque la API tiene algunos datos de bounces, puede que falten detalles
        como razones espec√≠ficas, c√≥digos de error, etc.
        
        Args:
            campaign_id: ID de la campa√±a
            
        Returns:
            Lista de hard bounces con informaci√≥n detallada
            
        TODO: IMPLEMENTAR DESPU√âS DE get_non_openers()
        """
        logger.info(f"üîç Iniciando scraping de hard bounces para campa√±a {campaign_id}")
        
        def _scrape_hard_bounces():
            # TODO: 1. Navegar a la secci√≥n de hard bounces
            self.navigate_to_campaign(campaign_id)
            
            # TODO: 2. Buscar secci√≥n de bounces/errores
            # self.navigation.go_to_campaign_tab(campaign_id, "bounces")
            
            # TODO: 3. Extraer hard bounces con detalles
            all_hard_bounces = []
            
            def extract_bounces_from_page(elements):
                """TODO: Extraer hard bounces de la p√°gina"""
                page_bounces = []
                
                for element in elements:
                    try:
                        # TODO: Extraer informaci√≥n del bounce:
                        # - Email
                        # - Fecha del bounce  
                        # - Raz√≥n espec√≠fica
                        # - C√≥digo de error
                        # - Detalles adicionales
                        
                        # bounce = ScrapedHardBounce(
                        #     email=self._extract_bounce_email(element),
                        #     campaign_id=campaign_id,
                        #     bounce_date=self._extract_bounce_date(element),
                        #     bounce_reason=self._extract_bounce_reason(element),
                        #     bounce_code=self._extract_bounce_code(element)
                        # )
                        # page_bounces.append(bounce)
                        
                        pass  # PLACEHOLDER
                        
                    except Exception as e:
                        logger.warning(f"Error extrayendo bounce: {e}")
                        continue
                
                return page_bounces
            
            # TODO: 4. Usar paginaci√≥n si es necesario
            # all_hard_bounces = self.handle_pagination(...)
            
            # PLACEHOLDER
            logger.warning("‚ö†Ô∏è  get_hard_bounces() NO IMPLEMENTADO - retornando lista vac√≠a")
            return all_hard_bounces
        
        return self.wait_and_retry(_scrape_hard_bounces)
    
    # === M√âTODO AVANZADO: ESTAD√çSTICAS EXTENDIDAS ===
    def get_extended_stats(self, campaign_id: int) -> ScrapedCampaignStats:
        """
        üìä Obtener estad√≠sticas avanzadas no disponibles en API
        
        Incluye datos como:
        - Estad√≠sticas geogr√°ficas (por pa√≠s)
        - Estad√≠sticas por dispositivo
        - Distribuci√≥n por horarios
        
        Args:
            campaign_id: ID de la campa√±a
            
        Returns:
            Estad√≠sticas extendidas de la campa√±a
            
        TODO: IMPLEMENTAR COMO √öLTIMO (OPCIONAL)
        """
        logger.info(f"üìä Iniciando scraping de estad√≠sticas extendidas para campa√±a {campaign_id}")
        start_time = time.time()
        
        def _scrape_extended_stats():
            # TODO: 1. Navegar a secci√≥n de estad√≠sticas avanzadas
            self.navigate_to_campaign(campaign_id)
            
            # TODO: 2. Extraer estad√≠sticas b√°sicas como verificaci√≥n
            stats = ScrapedCampaignStats(campaign_id=campaign_id)
            
            # TODO: 3. Extraer n√∫meros b√°sicos
            # stats.total_sent = self._extract_stat_number(self.selectors.total_sent)
            # stats.total_opened = self._extract_stat_number(self.selectors.total_opened)
            # stats.total_not_opened = self._extract_stat_number(self.selectors.total_not_opened)
            
            # TODO: 4. Extraer estad√≠sticas geogr√°ficas
            # stats.geographic_stats = self._scrape_geographic_stats()
            
            # TODO: 5. Extraer estad√≠sticas por dispositivo  
            # stats.device_stats = self._scrape_device_stats()
            
            # TODO: 6. Extraer estad√≠sticas temporales (hora/d√≠a)
            # stats.hourly_stats = self._scrape_hourly_stats()
            # stats.daily_stats = self._scrape_daily_stats()
            
            # Metadatos
            stats.scraped_at = datetime.now().isoformat()
            stats.scraping_duration = time.time() - start_time
            
            # PLACEHOLDER
            logger.warning("‚ö†Ô∏è  get_extended_stats() NO IMPLEMENTADO - retornando stats vac√≠as")
            return stats
        
        return self.wait_and_retry(_scrape_extended_stats)
    
    # === M√âTODO: URLS DE CAMPA√ëA ===
    def get_campaign_urls(self, campaign_id: int) -> List[ScrapedCampaignUrl]:
        """
        üîó Obtener URLs de la campa√±a con estad√≠sticas de clics

        Navega a la p√°gina de seguimiento de URLs y extrae:
        - URL completa
        - N√∫mero total de clics
        - Porcentaje de abridores que hicieron clic

        Args:
            campaign_id: ID de la campa√±a

        Returns:
            Lista de URLs con sus estad√≠sticas de clics
        """
        logger.info(f"üîó Iniciando scraping de URLs para campa√±a {campaign_id}")
        start_time = time.time()

        def _scrape_urls():
            campaign_urls = []

            try:
                # 1. Navegar a la p√°gina de seguimiento de URLs
                url_page = f"https://acumbamail.com/report/campaign/{campaign_id}/url/"
                logger.info(f"üåê Navegando a: {url_page}")
                self.page.goto(url_page, wait_until="networkidle", timeout=60000)

                # Esperar a que la p√°gina cargue completamente
                self.page.wait_for_load_state("networkidle", timeout=30000)
                self.page.wait_for_timeout(1000)  # Espera adicional para estabilidad

                # 2. Buscar la lista de URLs
                # Bas√°ndonos en el snapshot de BrowserMCP, las URLs est√°n en una lista (<ul> o <ol>)
                # Primero intentamos encontrar la lista principal
                try:
                    # Esperar a que aparezca al menos un listitem con URL
                    self.page.wait_for_selector("ul li, ol li", timeout=10000)
                    logger.info("‚úÖ Lista de URLs encontrada")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  No se encontraron URLs en la campa√±a {campaign_id}: {e}")
                    return []

                # 3. Contar items primero
                list_items_count = self.page.locator("ul li, ol li").count()
                logger.info(f"üìä Se encontraron {list_items_count} items en la lista")

                # Filtrar solo los items que contienen URLs (ignorar encabezados)
                # Procesar item por item, volviendo a la p√°gina principal despu√©s de cada navegaci√≥n
                for item_index in range(list_items_count):
                    # Re-obtener el item en cada iteraci√≥n para evitar elementos obsoletos
                    item = self.page.locator("ul li, ol li").nth(item_index)
                    try:
                        # Obtener el texto completo del item
                        item_text = item.text_content()

                        if not item_text:
                            continue

                        # Filtrar el encabezado "Url Han hecho clic Acciones"
                        if "Url Han hecho clic Acciones" in item_text or "Han hecho clic" in item_text:
                            continue

                        # Buscar enlaces dentro del item - pueden contener la URL completa en href o title
                        import re
                        url = None

                        # M√©todo 1: Buscar enlace <a> con href (URLs externas directas)
                        links_locator = item.locator('a')
                        links_count = links_locator.count()

                        for link_idx in range(links_count):
                            link = links_locator.nth(link_idx)
                            href = link.get_attribute('href')
                            # Si encuentra URL externa directa (poco com√∫n)
                            if href and href.startswith('http'):
                                url = href
                                break

                        # M√©todo 2: Buscar enlace "Detalles" y extraer URL de la p√°gina de detalles
                        if not url:
                            # Buscar enlace que contiene "Detalles" o "/click/.../details/"
                            details_link = None
                            for link_idx in range(links_count):
                                link = links_locator.nth(link_idx)
                                href = link.get_attribute('href')
                                link_text = link.text_content().strip()
                                if href and ('/click/' in href and '/details/' in href) or link_text == 'Detalles':
                                    details_link = href
                                    break

                            if details_link:
                                # Navegar a la p√°gina de detalles para obtener la URL completa
                                try:
                                    # Construir URL completa si es relativa
                                    if details_link.startswith('/'):
                                        details_url = f"https://acumbamail.com{details_link}"
                                    else:
                                        details_url = details_link

                                    logger.debug(f"   üîç Navegando a detalles: {details_url}")

                                    # Guardar URL actual para volver
                                    current_url = self.page.url

                                    # Navegar a detalles
                                    self.page.goto(details_url, wait_until="domcontentloaded", timeout=15000)
                                    self.page.wait_for_timeout(500)

                                    # Buscar la URL completa en la p√°gina de detalles
                                    # La URL suele estar en un elemento con la clase que indica la URL rastreada
                                    page_content = self.page.content()

                                    # Buscar URLs completas en el HTML
                                    url_patterns = [
                                        r'(https?://[^"\s<>]+)',  # URL entre comillas o espacios
                                    ]

                                    for pattern in url_patterns:
                                        urls_found = re.findall(pattern, page_content)
                                        for found_url in urls_found:
                                            # Filtrar URLs que NO son relevantes
                                            skip_domains = [
                                                'acumbamail.com',
                                                'clickacm.com',
                                                'w3.org',
                                                'schema.org',
                                                'google.com/recaptcha',
                                                'gstatic.com'
                                            ]

                                            # Verificar si la URL debe ser saltada
                                            should_skip = any(domain in found_url.lower() for domain in skip_domains)

                                            # Adem√°s, la URL debe ser similar a la truncada que vimos
                                            # Extraer el inicio de la URL truncada del item_text
                                            truncated_match = re.search(r'(https?://[^\s]{20,})', item_text)
                                            if truncated_match:
                                                truncated_url_start = truncated_match.group(1)[:50]  # Primeros 50 caracteres
                                                # La URL encontrada debe empezar similar a la truncada
                                                if not should_skip and found_url.startswith(truncated_url_start[:30]):
                                                    url = found_url
                                                    logger.debug(f"   ‚úÖ URL completa encontrada en detalles: {url[:80]}...")
                                                    break

                                        if url:
                                            break

                                    # Volver a la p√°gina de lista de URLs
                                    self.page.goto(current_url, wait_until="networkidle", timeout=15000)
                                    self.page.wait_for_timeout(1000)  # Esperar m√°s tiempo para estabilidad

                                except Exception as e:
                                    logger.warning(f"   ‚ö†Ô∏è Error obteniendo URL desde detalles: {e}")

                        # M√©todo 3: Extraer del texto visible (√∫ltimo recurso, puede estar truncada)
                        if not url:
                            url_match = re.search(r'(https?://\S+)', item_text)
                            if url_match:
                                url = url_match.group(1)
                                logger.debug(f"   ‚ö†Ô∏è Usando URL del texto (puede estar truncada): {url}")
                            else:
                                continue

                        # Patr√≥n para buscar clics: "X (Y% abridores)" o "X (Y,Y% abridores)"
                        clicks_match = re.search(r'(\d+)\s*\((\d+[,.]?\d*)\s*%\s*abridores\)', item_text)

                        if clicks_match:
                            clicks = int(clicks_match.group(1))
                            percentage_str = clicks_match.group(2).replace(',', '.')
                            percentage = float(percentage_str)
                        else:
                            # Si no se encuentra el patr√≥n, intentar buscar solo el n√∫mero
                            clicks_simple = re.search(r'(\d+)\s+\(', item_text)
                            if clicks_simple:
                                clicks = int(clicks_simple.group(1))
                                percentage = 0.0
                            else:
                                clicks = 0
                                percentage = 0.0

                        # Crear objeto ScrapedCampaignUrl
                        campaign_url = ScrapedCampaignUrl(
                            url=url,
                            clicks=clicks,
                            click_percentage=percentage,
                            campaign_id=campaign_id
                        )

                        campaign_urls.append(campaign_url)
                        logger.debug(f"   ‚úÖ URL extra√≠da: {campaign_url.short_url} - {clicks} clics ({percentage}%)")

                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è  Error procesando item de URL: {e}")
                        continue

                duration = time.time() - start_time
                logger.info(f"‚úÖ URLs scraping completado: {len(campaign_urls)} URLs en {duration:.1f}s")
                return campaign_urls

            except Exception as e:
                logger.error(f"‚ùå Error en scraping de URLs: {e}")
                return []

        return self.wait_and_retry(_scrape_urls)

    # === M√âTODO COMBINADO ===
    def get_complete_campaign_data(self, campaign_id: int,
                                 include_non_openers: bool = True,
                                 include_hard_bounces: bool = True,
                                 include_extended_stats: bool = False,
                                 include_campaign_urls: bool = True) -> ScrapedCampaignData:
        """
        üéØ Obtener todos los datos scrapeados de una campa√±a

        Este m√©todo combina todos los otros m√©todos para obtener
        un conjunto completo de datos.

        Args:
            campaign_id: ID de la campa√±a
            include_non_openers: Si incluir no-openers (recomendado: True)
            include_hard_bounces: Si incluir hard bounces (recomendado: True)
            include_extended_stats: Si incluir stats extendidas (opcional: False)
            include_campaign_urls: Si incluir URLs de campa√±a (recomendado: True)

        Returns:
            Objeto con todos los datos scrapeados
        """
        logger.info(f"üéØ Iniciando scraping completo de campa√±a {campaign_id}")
        start_time = time.time()
        
        # Inicializar contenedor de datos
        campaign_data = ScrapedCampaignData(
            campaign_id=campaign_id,
            scraped_at=datetime.now().isoformat()
        )
        
        # Obtener no-openers si se solicita
        if include_non_openers:
            try:
                logger.info("üìß Obteniendo no-openers...")
                campaign_data.non_openers = self.get_non_openers(campaign_id)
                campaign_data.scraping_methods.append("get_non_openers")
                logger.info(f"‚úÖ No-openers obtenidos: {len(campaign_data.non_openers)}")
            except Exception as e:
                logger.error(f"‚ùå Error obteniendo no-openers: {e}")
        
        # Obtener hard bounces si se solicita
        if include_hard_bounces:
            try:
                logger.info("üí• Obteniendo hard bounces...")
                campaign_data.hard_bounces = self.get_hard_bounces(campaign_id)
                campaign_data.scraping_methods.append("get_hard_bounces")
                logger.info(f"‚úÖ Hard bounces obtenidos: {len(campaign_data.hard_bounces)}")
            except Exception as e:
                logger.error(f"‚ùå Error obteniendo hard bounces: {e}")
        
        # Obtener URLs de campa√±a si se solicita
        if include_campaign_urls:
            try:
                logger.info("üîó Obteniendo URLs de campa√±a...")
                campaign_data.campaign_urls = self.get_campaign_urls(campaign_id)
                campaign_data.scraping_methods.append("get_campaign_urls")
                logger.info(f"‚úÖ URLs de campa√±a obtenidas: {len(campaign_data.campaign_urls)}")
            except Exception as e:
                logger.error(f"‚ùå Error obteniendo URLs de campa√±a: {e}")

        # Obtener estad√≠sticas extendidas si se solicita
        if include_extended_stats:
            try:
                logger.info("üìä Obteniendo estad√≠sticas extendidas...")
                campaign_data.extended_stats = self.get_extended_stats(campaign_id)
                campaign_data.scraping_methods.append("get_extended_stats")
                logger.info("‚úÖ Estad√≠sticas extendidas obtenidas")
            except Exception as e:
                logger.error(f"‚ùå Error obteniendo estad√≠sticas extendidas: {e}")

        duration = time.time() - start_time
        logger.info(f"üéâ Scraping completo finalizado en {duration:.1f}s")
        logger.info(f"üìä Resumen: {campaign_data.summary}")

        return campaign_data
    
    # === M√âTODOS AUXILIARES (IMPLEMENTAR SEG√öN NECESIDAD) ===
    
    def _extract_stat_number(self, selector: str) -> int:
        """
        TODO: Extraer n√∫mero de un elemento estad√≠stico
        
        Args:
            selector: Selector CSS del elemento
            
        Returns:
            N√∫mero extra√≠do o 0 si no se encuentra
        """
        try:
            text = self.get_text_content(selector, "0")
            # TODO: Limpiar texto y convertir a n√∫mero
            # Ejemplo: "1,234 emails" -> 1234
            clean_text = text.replace(",", "").replace(".", "")
            numbers = ''.join(filter(str.isdigit, clean_text))
            return int(numbers) if numbers else 0
        except Exception as e:
            logger.warning(f"Error extrayendo n√∫mero de '{selector}': {e}")
            return 0
    
    def _extract_date_sent(self, element) -> Optional[str]:
        """TODO: Extraer fecha de env√≠o de un elemento"""
        # TODO: Implementar seg√∫n estructura HTML real
        return None
    
    def _extract_subscriber_name(self, element) -> Optional[str]:
        """TODO: Extraer nombre del suscriptor de un elemento"""
        # TODO: Implementar seg√∫n estructura HTML real  
        return None
    
    def _extract_list_name(self, element) -> Optional[str]:
        """TODO: Extraer nombre de la lista de un elemento"""
        # TODO: Implementar seg√∫n estructura HTML real
        return None
    
    def _scrape_geographic_stats(self) -> List[ScrapedGeographicStats]:
        """TODO: Extraer estad√≠sticas geogr√°ficas"""
        # TODO: Buscar secci√≥n de estad√≠sticas por pa√≠s
        # TODO: Extraer datos de cada pa√≠s
        logger.warning("‚ö†Ô∏è  _scrape_geographic_stats() NO IMPLEMENTADO")
        return []
    
    def _scrape_device_stats(self) -> List[ScrapedDeviceStats]:
        """TODO: Extraer estad√≠sticas por dispositivo"""
        # TODO: Buscar secci√≥n de estad√≠sticas por dispositivo
        # TODO: Extraer datos de cada tipo de dispositivo
        logger.warning("‚ö†Ô∏è  _scrape_device_stats() NO IMPLEMENTADO")
        return []
    
    def _scrape_hourly_stats(self) -> Dict[str, int]:
        """TODO: Extraer estad√≠sticas por hora del d√≠a"""
        logger.warning("‚ö†Ô∏è  _scrape_hourly_stats() NO IMPLEMENTADO")
        return {}
    
    def _scrape_daily_stats(self) -> Dict[str, int]:
        """TODO: Extraer estad√≠sticas por d√≠a de la semana"""
        logger.warning("‚ö†Ô∏è  _scrape_daily_stats() NO IMPLEMENTADO")
        return {}


# === INSTRUCCIONES DE IMPLEMENTACI√ìN ===
"""
üöÄ GU√çA DE IMPLEMENTACI√ìN:

1Ô∏è‚É£ PRIMERO: Implementar get_non_openers()
   - Es el m√©todo m√°s importante
   - Actualizar selectores en selectors.py
   - Probar con una campa√±a real
   - Manejar paginaci√≥n

2Ô∏è‚É£ SEGUNDO: Implementar get_hard_bounces()  
   - Similar a no-openers pero con m√°s detalles
   - Extraer razones de bounce
   - C√≥digos de error si est√°n disponibles

3Ô∏è‚É£ TERCERO: Implementar get_extended_stats() (opcional)
   - Estad√≠sticas avanzadas
   - Datos geogr√°ficos y por dispositivo
   - Solo si necesitas estos datos

üìù PASOS PARA CADA M√âTODO:

1. Inspeccionar la p√°gina web real en el navegador
2. Anotar los selectores CSS correctos en selectors.py
3. Implementar la l√≥gica de extracci√≥n
4. Manejar errores y casos edge
5. Probar con datos reales
6. Optimizar rendimiento

üîß TIPS:
- Usa screenshots autom√°ticos en errores para debug
- Implementa timeouts apropiados
- Maneja la paginaci√≥n cuidadosamente  
- Loguea todo para debugging
- Prueba con diferentes campa√±as
"""