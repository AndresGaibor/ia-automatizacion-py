import sys
from pathlib import Path

# Configurar package para imports consistentes y PyInstaller compatibility
if __package__ in (None, ""):
    sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
    __package__ = "src"

from .excel_utils import agregar_datos, crear_o_cargar_libro_excel, obtener_o_crear_hoja, limpiar_hoja_desde_fila
from .shared.utils.legacy_utils import data_path, notify
from .shared.logging.logger import get_logger
from .utils import (
    crear_contexto_navegador,
    configurar_navegador,
    navegar_a_reportes,
    obtener_total_paginas,
    navegar_siguiente_pagina,
)
from .autentificacion import login
from .infrastructure.api import API

from playwright.sync_api import sync_playwright, Page
import re

# Rutas
ARCHIVO_BUSQUEDA = data_path("Busqueda.xlsx")

logger = get_logger()


def extraer_id_de_url(url: str) -> str:
    """
    Extrae el ID de campaÃ±a de una URL

    Ejemplo: /report/campaign/12345/ -> 12345
    """
    match = re.search(r'/campaign/(\d+)', url)
    if match:
        return match.group(1)
    return ""


def extraer_datos_campania_de_listitem(listitem_locator, page: Page) -> list[str]:
    """
    Extrae los datos de una campaÃ±a desde un listitem de la lista de informes

    Args:
        listitem_locator: Locator del listitem de la pÃ¡gina de informes
        page: PÃ¡gina de Playwright

    Returns:
        Lista con los datos: ['', nombre, id, fecha, total_enviado, abierto, clics]
    """
    try:
        logger.debug("ğŸ” Iniciando extracciÃ³n de datos de listitem")

        # Obtener el primer link que apunta a /report/campaign/ID/
        # Usando locators modernos
        campaign_link = listitem_locator.locator('a[href*="/report/campaign/"]').first

        # Verificar si existe
        if campaign_link.count() == 0:
            logger.warning("âš ï¸ No se encontrÃ³ link de campaÃ±a en el listitem")
            return []

        # El primer link es el nombre de la campaÃ±a
        nombre = campaign_link.inner_text().strip()
        href = campaign_link.get_attribute('href') or ""
        id_campania = extraer_id_de_url(href)

        logger.debug(f"ğŸ“ Nombre extraÃ­do: {nombre}, ID: {id_campania}")

        # Obtener todo el texto del listitem
        full_text = listitem_locator.inner_text()
        logger.debug(f"ğŸ“„ Texto completo del listitem: {full_text[:200]}...")

        # El texto despuÃ©s del nombre contiene: Tipo Fecha Listas Emails Abiertos Clics
        # Ejemplo: "20251010_Com_Novedades_SIRAJ2 ClÃ¡sica 10/10/25 08:32 Equipo_Minsait , ... 8.140 2.426 0"

        # Extraer fecha (formato DD/MM/YY HH:MM o DD/MM/YY)
        fecha_match = re.search(r'(\d{2}/\d{2}/\d{2})\s*(\d{2}:\d{2})?', full_text)
        if fecha_match:
            fecha = fecha_match.group(1)
            if fecha_match.group(2):
                fecha = f"{fecha} {fecha_match.group(2)}"
        else:
            fecha = ""

        logger.debug(f"ğŸ“… Fecha extraÃ­da: {fecha}")

        # Extraer los nÃºmeros al final (Emails, Abiertos, Clics)
        # Estrategia: buscar el Ãºltimo bloque de texto que contiene solo nÃºmeros separados por espacios
        # Ejemplo: "... Lista1 , Lista2 8.140 2.426 0" -> queremos "8.140 2.426 0"

        # Primero, eliminar todos los links para quedarnos solo con el texto
        links = listitem_locator.locator('a').all()
        text_only = full_text
        for link in links:
            link_text = link.inner_text()
            text_only = text_only.replace(link_text, '')

        # Ahora buscar los Ãºltimos nÃºmeros (despuÃ©s de la fecha)
        # PatrÃ³n: buscar grupos de 3 nÃºmeros al final (pueden tener puntos como separadores)
        match = re.search(r'(\d{1,3}(?:\.\d{3})*|\d+)\s+(\d{1,3}(?:\.\d{3})*|\d+)\s+(\d{1,3}(?:\.\d{3})*|\d+)\s*$', text_only)

        if match:
            total_enviado = match.group(1).replace('.', '')
            abierto = match.group(2).replace('.', '')
            clics = match.group(3).replace('.', '')
            logger.debug(f"ğŸ”¢ NÃºmeros extraÃ­dos: Enviados={total_enviado}, Abiertos={abierto}, Clics={clics}")
        else:
            logger.warning(f"âš ï¸ No se encontrÃ³ el patrÃ³n de nÃºmeros al final del texto")
            logger.debug(f"Texto sin links: {text_only[:200]}...")
            total_enviado = "0"
            abierto = "0"
            clics = "0"

        # Calcular "No abierto"
        try:
            no_abierto = str(int(total_enviado) - int(abierto))
        except:
            no_abierto = "0"

        logger.debug(
            f"âœ… Datos extraÃ­dos exitosamente",
            extra={
                "nombre": nombre,
                "id": id_campania,
                "fecha": fecha,
                "total_enviado": total_enviado,
                "abierto": abierto,
                "no_abierto": no_abierto,
                "clics": clics
            }
        )

        return ['', nombre, id_campania, fecha, total_enviado, abierto, no_abierto]

    except Exception as e:
        logger.error(f"âŒ Error extrayendo datos de campaÃ±a: {e}", extra={"error": str(e)})
        return []


def extraer_campanias_de_pagina(page: Page) -> list[list[str]]:
    """
    Extrae todas las campaÃ±as de la pÃ¡gina actual de informes

    Args:
        page: PÃ¡gina de Playwright

    Returns:
        Lista de campaÃ±as con sus datos
    """
    logger.info("ğŸ” Iniciando extracciÃ³n de campaÃ±as de la pÃ¡gina actual")
    campanias = []

    try:
        # Esperar a que la lista se cargue
        logger.debug("â³ Esperando a que se cargue la lista de informes")
        page.wait_for_selector('ul li', timeout=15000)
        page.wait_for_timeout(1000)  # Espera adicional para asegurar carga completa

        logger.debug("âœ… Lista de informes cargada")

        # Usar selectores modernos de Playwright
        # Estrategia: buscar todos los li que contienen un link a /report/campaign/
        # y excluir el primero que es el encabezado
        all_items = page.locator('li').filter(has=page.locator('a[href*="/report/campaign/"]'))

        # Obtener el count
        count = all_items.count()
        logger.info(f"âœ… Total de elementos con links de campaÃ±as: {count}")

        # Obtener todos los elementos y filtrar manualmente los que son campaÃ±as reales
        # (excluir elementos anidados verificando que tengan texto con nÃºmeros al final)
        campaign_listitems = []
        for i in range(count):
            item = all_items.nth(i)
            text = item.inner_text()
            # Verificar que el texto contenga un patrÃ³n de nÃºmeros al final
            # (esto indica que es una fila de campaÃ±a, no un elemento anidado)
            if re.search(r'\d+\s+\d+\s+\d+\s*$', text):
                campaign_listitems.append(item)

        logger.info(f"âœ… Listitems de campaÃ±as reales encontrados: {len(campaign_listitems)}")

        for i, listitem in enumerate(campaign_listitems, 1):
            try:
                logger.debug(f"ğŸ“– Procesando campaÃ±a {i}/{len(campaign_listitems)}")
                datos = extraer_datos_campania_de_listitem(listitem, page)

                if datos:  # Solo agregar si se extrajeron datos vÃ¡lidos
                    campanias.append(datos)
                    logger.info(f"âœ… CampaÃ±a {i} extraÃ­da: {datos[1]} (ID: {datos[2]})")
                else:
                    logger.warning(f"âš ï¸ No se pudieron extraer datos de la campaÃ±a {i}")

            except Exception as e:
                logger.warning(f"âš ï¸ Error procesando listitem {i}: {e}")
                continue

        logger.success(f"âœ… ExtracciÃ³n completada: {len(campanias)} campaÃ±as extraÃ­das de la pÃ¡gina")

    except Exception as e:
        logger.error(f"âŒ Error extrayendo campaÃ±as de la pÃ¡gina: {e}")

    return campanias


def guardar_datos_en_excel(informe_detalle: list[list[str]], archivo_busqueda: str):
    """
    Guarda los datos en el archivo Excel, usando la primera hoja por defecto
    y ajusta automÃ¡ticamente el ancho de las columnas
    """
    try:
        logger.info("ğŸš€ Iniciando guardado de datos en Excel", extra={"archivo": archivo_busqueda, "registros": len(informe_detalle)})

        wb = crear_o_cargar_libro_excel(archivo_busqueda)
        encabezados = ["Buscar", "Nombre", "ID CampaÃ±a", "Fecha", "Total enviado", "Abierto", "No abierto"]

        # Obtener o crear la hoja "Sheet"
        ws = obtener_o_crear_hoja(wb, "Sheet")
        logger.info(f"ğŸ“ Hoja obtenida/creada: {ws.title}")

        # Limpiar hoja desde la primera fila
        logger.info("ğŸ§¹ Limpiando hoja")
        limpiar_hoja_desde_fila(ws, fila_inicial=1)

        # Agregar encabezados
        logger.info("ğŸ·ï¸ Agregando encabezados")
        ws.append(encabezados)

        # Agregar datos
        registros_agregados = agregar_datos(ws, datos=informe_detalle)
        logger.info(f"ğŸ“Š Datos agregados: {registros_agregados} registros")

        # Ajustar automÃ¡ticamente el ancho de las columnas
        from openpyxl.utils import get_column_letter

        # Iterar por cada columna usando Ã­ndices
        logger.info("ğŸ“ Ajustando ancho de columnas")
        for col_idx in range(1, ws.max_column + 1):
            max_length = 0
            column_letter = get_column_letter(col_idx)

            # Revisar todas las celdas de esta columna
            for row_idx in range(1, ws.max_row + 1):
                cell = ws.cell(row=row_idx, column=col_idx)
                try:
                    if cell.value and len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass

            # Ajustar el ancho (agregar un poco de padding)
            adjusted_width = min(max_length + 2, 50)  # MÃ¡ximo 50 caracteres
            ws.column_dimensions[column_letter].width = adjusted_width

        wb.save(archivo_busqueda)
        logger.success(f"âœ… Archivo guardado exitosamente: {archivo_busqueda}")
        logger.info(f"ğŸ“ˆ Se agregaron {registros_agregados} registros al archivo")

    except Exception as e:
        logger.error(f"âŒ Error guardando archivo Excel: {e}")
        print(f"Error guardando archivo Excel: {e}")


def procesar_todas_las_paginas(page: Page) -> list[list[str]]:
    """
    Procesa todas las pÃ¡ginas de reportes y extrae todas las campaÃ±as

    Args:
        page: PÃ¡gina de Playwright

    Returns:
        Lista con todos los datos de campaÃ±as
    """
    logger.info("ğŸ” Iniciando procesamiento de todas las pÃ¡ginas")
    todas_campanias = []

    try:
        # Obtener nÃºmero total de pÃ¡ginas
        logger.debug("ğŸ“„ Obteniendo nÃºmero total de pÃ¡ginas")
        total_paginas = obtener_total_paginas(page)
        logger.info(f"ğŸ“š Total de pÃ¡ginas a procesar: {total_paginas}")

        # Procesar cada pÃ¡gina
        for pagina_actual in range(1, total_paginas + 1):
            logger.info(f"ğŸ“– === PROCESANDO PÃGINA {pagina_actual} DE {total_paginas} ===")

            # Extraer campaÃ±as de la pÃ¡gina actual
            campanias_pagina = extraer_campanias_de_pagina(page)
            todas_campanias.extend(campanias_pagina)

            logger.info(
                f"âœ… PÃ¡gina {pagina_actual} completada",
                extra={
                    "campanias_en_pagina": len(campanias_pagina),
                    "total_acumulado": len(todas_campanias)
                }
            )

            # Navegar a la siguiente pÃ¡gina si no es la Ãºltima
            if pagina_actual < total_paginas:
                logger.debug(f"â¡ï¸ Navegando a pÃ¡gina {pagina_actual + 1}")
                exito = navegar_siguiente_pagina(page, pagina_actual)
                if not exito:
                    logger.warning(f"âš ï¸ No se pudo navegar a pÃ¡gina {pagina_actual + 1}, finalizando procesamiento")
                    break

                # Espera adicional despuÃ©s de navegar
                page.wait_for_timeout(2000)
                logger.debug(f"âœ… NavegaciÃ³n a pÃ¡gina {pagina_actual + 1} completada")

        logger.success(f"ğŸ‰ Procesamiento completo: {len(todas_campanias)} campaÃ±as extraÃ­das de {pagina_actual} pÃ¡ginas")

    except Exception as e:
        logger.error(f"âŒ Error procesando pÃ¡ginas: {e}")

    return todas_campanias


def main():
    """
    FunciÃ³n principal del programa de listado de campaÃ±as
    Usa API para obtener IDs y scraping para completar los datos
    """
    logger.info("ğŸš€ Iniciando programa de listado de campaÃ±as (modo hÃ­brido: API + Scraping)")

    try:
        with sync_playwright() as p:
            # Configurar navegador
            logger.info("ğŸŒ Configurando navegador Playwright")
            browser = configurar_navegador(p, extraccion_oculta=False)
            context = crear_contexto_navegador(browser, extraccion_oculta=False)
            page = context.new_page()
            logger.success("âœ… Navegador configurado correctamente")

            # Login
            logger.info("ğŸ” Iniciando proceso de autenticaciÃ³n")
            login(page, context)
            logger.success("âœ… SesiÃ³n iniciada correctamente")

            # Navegar a reportes
            logger.info("ğŸ“Š Navegando a secciÃ³n de reportes")
            navegar_a_reportes(page)
            logger.success("âœ… NavegaciÃ³n a reportes completada")

            # Esperar a que la pÃ¡gina cargue completamente
            logger.debug("â³ Esperando carga completa de la pÃ¡gina")
            page.wait_for_load_state("networkidle", timeout=60000)
            page.wait_for_timeout(3000)  # Espera adicional para asegurar carga completa
            logger.debug("âœ… PÃ¡gina cargada completamente")

            # Procesar todas las pÃ¡ginas y extraer campaÃ±as con scraping
            logger.info("ğŸ“¥ Iniciando extracciÃ³n de campaÃ±as mediante scraping")
            informe = procesar_todas_las_paginas(page)
            logger.info(f"ğŸ“Š Total de campaÃ±as extraÃ­das mediante scraping: {len(informe)}")

            # Guardar en Excel
            if informe:
                logger.info("ğŸ’¾ Guardando datos en archivo Excel")
                guardar_datos_en_excel(informe, ARCHIVO_BUSQUEDA)
                logger.success("âœ… Programa completado exitosamente")
                notify("Listado de CampaÃ±as", f"Se extrajeron {len(informe)} campaÃ±as correctamente", "info")
            else:
                logger.warning("âš ï¸ No se encontraron campaÃ±as para guardar")
                notify("Listado de CampaÃ±as", "No se encontraron campaÃ±as", "warning")

            # Cerrar navegador
            logger.debug("ğŸ”š Cerrando navegador")
            browser.close()
            logger.info("âœ… Navegador cerrado correctamente")

    except Exception as e:
        logger.error(f"âŒ Error crÃ­tico en el programa: {e}", extra={"error": str(e)})
        print(f"Error crÃ­tico en el programa: {e}")
        notify("Error", f"Error crÃ­tico: {e}", "error")
        raise


if __name__ == "__main__":
    main()
