import sys
from pathlib import Path

# Configurar package para imports consistentes y PyInstaller compatibility
if __package__ in (None, ""):
    sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
    __package__ = "src"

from .excel_utils import agregar_datos, crear_o_cargar_libro_excel, obtener_o_crear_hoja, limpiar_hoja_desde_fila
from .shared.utils.legacy_utils import data_path, notify
from .shared.logging.logger import get_logger
from .utils import (
    crear_contexto_navegador,
    configurar_navegador,
    navegar_a_reportes,
    obtener_total_paginas,
    navegar_siguiente_pagina,
)
from .autentificacion import login
from .infrastructure.api import API

from playwright.sync_api import sync_playwright, Page
import re

# Rutas
ARCHIVO_BUSQUEDA = data_path("Busqueda.xlsx")

logger = get_logger()


def extraer_id_de_url(url: str) -> str:
    """
    Extrae el ID de campa√±a de una URL

    Ejemplo: /report/campaign/12345/ -> 12345
    """
    match = re.search(r'/campaign/(\d+)', url)
    if match:
        return match.group(1)
    return ""


def extraer_datos_campania_de_listitem(listitem_locator, page: Page) -> list[str]:
    """
    Extrae los datos de una campa√±a desde un listitem de la lista de informes

    Args:
        listitem_locator: Locator del listitem de la p√°gina de informes
        page: P√°gina de Playwright

    Returns:
        Lista con los datos: ['', nombre, id, fecha, total_enviado, abierto, clics]
    """
    try:
        logger.debug("üîç Iniciando extracci√≥n de datos de listitem")

        # Obtener el primer link que apunta a /report/campaign/ID/
        # Usando locators modernos
        campaign_link = listitem_locator.locator('a[href*="/report/campaign/"]').first

        # Verificar si existe
        if campaign_link.count() == 0:
            logger.warning("‚ö†Ô∏è No se encontr√≥ link de campa√±a en el listitem")
            return []

        # El primer link es el nombre de la campa√±a
        nombre = campaign_link.inner_text().strip()
        href = campaign_link.get_attribute('href') or ""
        id_campania = extraer_id_de_url(href)

        logger.debug(f"üìù Nombre extra√≠do: {nombre}, ID: {id_campania}")

        # Obtener todo el texto del listitem
        full_text = listitem_locator.inner_text()
        logger.debug(f"üìÑ Texto completo del listitem: {full_text[:200]}...")

        # El texto despu√©s del nombre contiene: Tipo Fecha Listas Emails Abiertos Clics
        # Ejemplo: "20251010_Com_Novedades_SIRAJ2 Cl√°sica 10/10/25 08:32 Equipo_Minsait , ... 8.140 2.426 0"

        # Extraer fecha (formato DD/MM/YY HH:MM o DD/MM/YY)
        fecha_match = re.search(r'(\d{2}/\d{2}/\d{2})\s*(\d{2}:\d{2})?', full_text)
        if fecha_match:
            fecha = fecha_match.group(1)
            if fecha_match.group(2):
                fecha = f"{fecha} {fecha_match.group(2)}"
        else:
            fecha = ""

        logger.debug(f"üìÖ Fecha extra√≠da: {fecha}")

        # Extraer los n√∫meros al final (Emails, Abiertos, Clics)
        # Estrategia: buscar el √∫ltimo bloque de texto que contiene solo n√∫meros separados por espacios
        # Ejemplo: "... Lista1 , Lista2 8.140 2.426 0" -> queremos "8.140 2.426 0"

        # Primero, eliminar todos los links para quedarnos solo con el texto
        links = listitem_locator.locator('a').all()
        text_only = full_text
        for link in links:
            link_text = link.inner_text()
            text_only = text_only.replace(link_text, '')

        # Ahora buscar los √∫ltimos n√∫meros (despu√©s de la fecha)
        # Patr√≥n: buscar grupos de 3 n√∫meros al final (pueden tener puntos como separadores)
        match = re.search(r'(\d{1,3}(?:\.\d{3})*|\d+)\s+(\d{1,3}(?:\.\d{3})*|\d+)\s+(\d{1,3}(?:\.\d{3})*|\d+)\s*$', text_only)

        if match:
            total_enviado = match.group(1).replace('.', '')
            abierto = match.group(2).replace('.', '')
            clics = match.group(3).replace('.', '')
            logger.debug(f"üî¢ N√∫meros extra√≠dos: Enviados={total_enviado}, Abiertos={abierto}, Clics={clics}")
        else:
            logger.warning(f"‚ö†Ô∏è No se encontr√≥ el patr√≥n de n√∫meros al final del texto")
            logger.debug(f"Texto sin links: {text_only[:200]}...")
            total_enviado = "0"
            abierto = "0"
            clics = "0"

        # Calcular "No abierto"
        try:
            no_abierto = str(int(total_enviado) - int(abierto))
        except:
            no_abierto = "0"

        logger.debug(
            f"‚úÖ Datos extra√≠dos exitosamente",
            extra={
                "nombre": nombre,
                "id": id_campania,
                "fecha": fecha,
                "total_enviado": total_enviado,
                "abierto": abierto,
                "no_abierto": no_abierto,
                "clics": clics
            }
        )

        return ['', nombre, id_campania, fecha, total_enviado, abierto, no_abierto]

    except Exception as e:
        logger.error(f"‚ùå Error extrayendo datos de campa√±a: {e}", extra={"error": str(e)})
        return []


def extraer_campanias_de_pagina(page: Page) -> list[list[str]]:
    """
    Extrae todas las campa√±as de la p√°gina actual de informes

    Args:
        page: P√°gina de Playwright

    Returns:
        Lista de campa√±as con sus datos (sin duplicados por ID)
    """
    logger.info("üîç Iniciando extracci√≥n de campa√±as de la p√°gina actual")
    campanias = []
    ids_vistos = set()  # Para evitar duplicados

    try:
        # Esperar a que la lista se cargue
        logger.debug("‚è≥ Esperando a que se cargue la lista de informes")
        page.wait_for_selector('ul li', timeout=15000)
        page.wait_for_timeout(1000)  # Espera adicional para asegurar carga completa

        logger.debug("‚úÖ Lista de informes cargada")

        # Usar selectores modernos de Playwright
        # Estrategia: buscar todos los li que contienen un link a /report/campaign/
        # y excluir el primero que es el encabezado
        all_items = page.locator('li').filter(has=page.locator('a[href*="/report/campaign/"]'))

        # Obtener el count
        count = all_items.count()
        logger.info(f"‚úÖ Total de elementos con links de campa√±as: {count}")

        # Obtener todos los elementos y filtrar manualmente los que son campa√±as reales
        # Estrategia mejorada: buscar elementos que tengan el patr√≥n completo de una fila de campa√±a
        campaign_listitems = []
        for i in range(count):
            item = all_items.nth(i)
            text = item.inner_text()

            # Criterios para identificar una fila de campa√±a real:
            # 1. Debe tener una fecha en formato DD/MM/YY (obligatorio - identifica campa√±as reales)
            # 2. Debe tener al menos 1 n√∫mero al final (pueden ser 0 0 0, o 1234, etc.)
            # 3. Debe tener longitud suficiente y no ser solo un fragmento
            tiene_fecha = bool(re.search(r'\d{2}/\d{2}/\d{2}', text))
            tiene_numeros_final = bool(re.search(r'\d+[\s,]*\d*[\s,]*\d*\s*$', text))
            longitud_suficiente = len(text.strip()) > 30

            # Verificar que NO sea un elemento anidado (no debe tener saltos de l√≠nea m√∫ltiples)
            no_es_anidado = text.count('\n') <= 3

            if tiene_fecha and tiene_numeros_final and longitud_suficiente and no_es_anidado:
                campaign_listitems.append(item)
                logger.debug(f"‚úÖ Campa√±a v√°lida encontrada en √≠ndice {i}: {text[:50]}...")
            else:
                # Logging detallado para debug - mostrar en consola los descartados
                if tiene_fecha and longitud_suficiente:  # Candidatos v√°lidos que fueron descartados
                    print(f"‚ö†Ô∏è DESCARTADO [{i}]: fecha={tiene_fecha}, numeros={tiene_numeros_final}, longitud={len(text.strip())}, saltos={text.count(chr(10))}")
                    print(f"   Texto: {text[:100]}")
                logger.debug(f"‚ö†Ô∏è Elemento descartado en √≠ndice {i}: tiene_fecha={tiene_fecha}, tiene_numeros={tiene_numeros_final}, longitud={len(text.strip())}, saltos_linea={text.count(chr(10))}")

        logger.info(f"‚úÖ Listitems de campa√±as reales encontrados: {len(campaign_listitems)}")

        for i, listitem in enumerate(campaign_listitems, 1):
            try:
                logger.debug(f"üìñ Procesando campa√±a {i}/{len(campaign_listitems)}")
                datos = extraer_datos_campania_de_listitem(listitem, page)

                if datos and len(datos) >= 3:  # Validar que tiene datos suficientes
                    id_campania = datos[2]  # El ID est√° en la posici√≥n 2

                    # Verificar si ya vimos este ID (evitar duplicados)
                    if id_campania in ids_vistos:
                        logger.warning(f"‚ö†Ô∏è Campa√±a duplicada detectada (ID: {id_campania}), omitiendo...")
                        continue

                    ids_vistos.add(id_campania)
                    campanias.append(datos)
                    logger.info(f"‚úÖ Campa√±a {len(campanias)} extra√≠da: {datos[1]} (ID: {datos[2]})")
                else:
                    logger.warning(f"‚ö†Ô∏è No se pudieron extraer datos v√°lidos de la campa√±a {i}")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error procesando listitem {i}: {e}")
                continue

        logger.success(f"‚úÖ Extracci√≥n completada: {len(campanias)} campa√±as √∫nicas extra√≠das de la p√°gina")

    except Exception as e:
        logger.error(f"‚ùå Error extrayendo campa√±as de la p√°gina: {e}")

    return campanias


def guardar_datos_en_excel(informe_detalle: list[list[str]], archivo_busqueda: str):
    """
    Guarda los datos en el archivo Excel, usando la primera hoja por defecto
    y ajusta autom√°ticamente el ancho de las columnas
    """
    try:
        logger.info("üöÄ Iniciando guardado de datos en Excel", extra={"archivo": archivo_busqueda, "registros": len(informe_detalle)})

        wb = crear_o_cargar_libro_excel(archivo_busqueda)
        encabezados = ["Buscar", "Nombre", "ID Campa√±a", "Fecha", "Total enviado", "Abierto", "No abierto"]

        # Obtener o crear la hoja "Sheet"
        ws = obtener_o_crear_hoja(wb, "Sheet")
        logger.info(f"üìù Hoja obtenida/creada: {ws.title}")

        # Limpiar hoja desde la primera fila
        logger.info("üßπ Limpiando hoja")
        limpiar_hoja_desde_fila(ws, fila_inicial=1)

        # Agregar encabezados
        logger.info("üè∑Ô∏è Agregando encabezados")
        ws.append(encabezados)

        # Agregar datos
        registros_agregados = agregar_datos(ws, datos=informe_detalle)
        logger.info(f"üìä Datos agregados: {registros_agregados} registros")

        # Ajustar autom√°ticamente el ancho de las columnas
        from openpyxl.utils import get_column_letter

        # Iterar por cada columna usando √≠ndices
        logger.info("üìê Ajustando ancho de columnas")
        for col_idx in range(1, ws.max_column + 1):
            max_length = 0
            column_letter = get_column_letter(col_idx)

            # Revisar todas las celdas de esta columna
            for row_idx in range(1, ws.max_row + 1):
                cell = ws.cell(row=row_idx, column=col_idx)
                try:
                    if cell.value and len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass

            # Ajustar el ancho (agregar un poco de padding)
            adjusted_width = min(max_length + 2, 50)  # M√°ximo 50 caracteres
            ws.column_dimensions[column_letter].width = adjusted_width

        wb.save(archivo_busqueda)
        logger.success(f"‚úÖ Archivo guardado exitosamente: {archivo_busqueda}")
        logger.info(f"üìà Se agregaron {registros_agregados} registros al archivo")

    except Exception as e:
        logger.error(f"‚ùå Error guardando archivo Excel: {e}")
        print(f"Error guardando archivo Excel: {e}")


def procesar_todas_las_paginas(page: Page) -> list[list[str]]:
    """
    Procesa todas las p√°ginas de reportes y extrae todas las campa√±as (sin duplicados globales)

    Args:
        page: P√°gina de Playwright

    Returns:
        Lista con todos los datos de campa√±as √∫nicas
    """
    logger.info("üîç Iniciando procesamiento de todas las p√°ginas")
    todas_campanias = []
    ids_globales = set()  # Para evitar duplicados entre p√°ginas

    try:
        # Obtener n√∫mero total de p√°ginas
        logger.debug("üìÑ Obteniendo n√∫mero total de p√°ginas")
        total_paginas = obtener_total_paginas(page)
        logger.info(f"üìö Total de p√°ginas a procesar: {total_paginas}")

        # Procesar cada p√°gina
        for pagina_actual in range(1, total_paginas + 1):
            logger.info(f"üìñ === PROCESANDO P√ÅGINA {pagina_actual} DE {total_paginas} ===")

            # Extraer campa√±as de la p√°gina actual
            campanias_pagina = extraer_campanias_de_pagina(page)

            # Filtrar duplicados globales (entre p√°ginas)
            campanias_nuevas = 0
            for campania in campanias_pagina:
                if len(campania) >= 3:
                    id_campania = campania[2]
                    if id_campania not in ids_globales:
                        ids_globales.add(id_campania)
                        todas_campanias.append(campania)
                        campanias_nuevas += 1
                    else:
                        logger.debug(f"‚ö†Ô∏è Campa√±a duplicada entre p√°ginas (ID: {id_campania}), omitiendo...")

            logger.info(
                f"‚úÖ P√°gina {pagina_actual} completada",
                extra={
                    "campanias_en_pagina": len(campanias_pagina),
                    "campanias_nuevas": campanias_nuevas,
                    "total_acumulado": len(todas_campanias)
                }
            )

            # Navegar a la siguiente p√°gina si no es la √∫ltima
            if pagina_actual < total_paginas:
                logger.debug(f"‚û°Ô∏è Navegando a p√°gina {pagina_actual + 1}")
                exito = navegar_siguiente_pagina(page, pagina_actual)
                if not exito:
                    logger.warning(f"‚ö†Ô∏è No se pudo navegar a p√°gina {pagina_actual + 1}, finalizando procesamiento")
                    break

                # Espera adicional despu√©s de navegar
                page.wait_for_timeout(2000)
                logger.debug(f"‚úÖ Navegaci√≥n a p√°gina {pagina_actual + 1} completada")

        logger.success(f"üéâ Procesamiento completo: {len(todas_campanias)} campa√±as extra√≠das de {pagina_actual} p√°ginas")

    except Exception as e:
        logger.error(f"‚ùå Error procesando p√°ginas: {e}")

    return todas_campanias


def main():
    """
    Funci√≥n principal del programa de listado de campa√±as
    Usa API para obtener IDs y scraping para completar los datos
    """
    logger.info("üöÄ Iniciando programa de listado de campa√±as (modo h√≠brido: API + Scraping)")

    try:
        with sync_playwright() as p:
            # Configurar navegador
            logger.info("üåê Configurando navegador Playwright")
            browser = configurar_navegador(p, extraccion_oculta=False)
            context = crear_contexto_navegador(browser, extraccion_oculta=False)
            page = context.new_page()
            logger.success("‚úÖ Navegador configurado correctamente")

            # Login
            logger.info("üîê Iniciando proceso de autenticaci√≥n")
            login(page, context)
            logger.success("‚úÖ Sesi√≥n iniciada correctamente")

            # Navegar a reportes
            logger.info("üìä Navegando a secci√≥n de reportes")
            navegar_a_reportes(page)
            logger.success("‚úÖ Navegaci√≥n a reportes completada")

            # Esperar a que la p√°gina cargue completamente
            logger.debug("‚è≥ Esperando carga completa de la p√°gina")
            page.wait_for_load_state("networkidle", timeout=60000)
            page.wait_for_timeout(3000)  # Espera adicional para asegurar carga completa
            logger.debug("‚úÖ P√°gina cargada completamente")

            # Procesar todas las p√°ginas y extraer campa√±as con scraping
            logger.info("üì• Iniciando extracci√≥n de campa√±as mediante scraping")
            informe = procesar_todas_las_paginas(page)
            logger.info(f"üìä Total de campa√±as extra√≠das mediante scraping: {len(informe)}")

            # Guardar en Excel
            if informe:
                logger.info("üíæ Guardando datos en archivo Excel")
                guardar_datos_en_excel(informe, ARCHIVO_BUSQUEDA)
                logger.success("‚úÖ Programa completado exitosamente")
                notify("Listado de Campa√±as", f"Se extrajeron {len(informe)} campa√±as correctamente", "info")
            else:
                logger.warning("‚ö†Ô∏è No se encontraron campa√±as para guardar")
                notify("Listado de Campa√±as", "No se encontraron campa√±as", "warning")

            # Cerrar navegador
            logger.debug("üîö Cerrando navegador")
            browser.close()
            logger.info("‚úÖ Navegador cerrado correctamente")

    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico en el programa: {e}", extra={"error": str(e)})
        print(f"Error cr√≠tico en el programa: {e}")
        notify("Error", f"Error cr√≠tico: {e}", "error")
        raise


if __name__ == "__main__":
    main()
