#!/usr/bin/env python3
"""
Script de prueba para el sistema de logging mejorado
Verifica que todos los componentes del logging funcionen correctamente
"""

import os
import sys
import tempfile
import json
from pathlib import Path

# Agregar src al path para importar mÃ³dulos
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.enhanced_logger import (
    get_logger, get_main_logger, get_auth_logger, get_browser_logger,
    get_crear_lista_logger, get_listar_campanias_logger, get_performance_logger,
    log_operation, log_errors
)

def test_basic_logging():
    """Prueba bÃ¡sica de funcionalidad de logging"""
    print("ğŸ§ª Probando logging bÃ¡sico...")

    logger = get_main_logger()

    # Pruebas bÃ¡sicas de logging
    logger.debug("Esta es una prueba de DEBUG")
    logger.info("Esta es una prueba de INFO")
    logger.warning("Esta es una prueba de WARNING")

    # Prueba de logging con contexto
    logger.info("Prueba con contexto", context={"user": "test", "action": "testing"})

    # Prueba de error simulado
    try:
        raise ValueError("Error de prueba")
    except Exception as e:
        logger.error("Error capturado en prueba", error=e, context={"test_type": "error_handling"})

    print("âœ… Logging bÃ¡sico completado")

def test_operation_context():
    """Prueba el contexto de operaciones"""
    print("ğŸ§ª Probando contexto de operaciones...")

    logger = get_main_logger()

    # Prueba de operaciÃ³n exitosa
    with logger.operation("test_operation_success", {"test_param": "value1"}) as op:
        op.log_progress("Iniciando operaciÃ³n de prueba")
        import time
        time.sleep(0.1)  # Simular trabajo
        op.log_progress("OperaciÃ³n en progreso", 50, 100)
        time.sleep(0.1)
        op.log_progress("OperaciÃ³n completÃ¡ndose", 100, 100)

    # Prueba de operaciÃ³n con error
    try:
        with logger.operation("test_operation_error", {"test_param": "value2"}) as op:
            op.log_progress("Iniciando operaciÃ³n que fallarÃ¡")
            time.sleep(0.1)
            raise RuntimeError("Error simulado en operaciÃ³n")
    except Exception:
        pass  # Error esperado

    print("âœ… Contexto de operaciones completado")

def test_specialized_loggers():
    """Prueba loggers especializados"""
    print("ğŸ§ª Probando loggers especializados...")

    # Logger de autenticaciÃ³n
    auth_logger = get_auth_logger()
    auth_logger.info("Prueba de autenticaciÃ³n", context={"username": "test_user"})

    # Logger de navegador
    browser_logger = get_browser_logger()
    browser_logger.log_browser_action("Navegando a pÃ¡gina", "https://test.com")

    # Logger de crear lista
    crear_logger = get_crear_lista_logger()
    crear_logger.log_file_operation("Leyendo", "/path/to/test.xlsx", 1024)

    # Logger de campaÃ±as
    campanias_logger = get_listar_campanias_logger()
    campanias_logger.log_data_extraction("campaÃ±as", 15, "base de datos")

    # Logger de rendimiento
    perf_logger = get_performance_logger()
    perf_logger.log_performance_metric("response_time", 250.5, "ms")

    print("âœ… Loggers especializados completados")

def test_decorators():
    """Prueba decoradores de logging"""
    print("ğŸ§ª Probando decoradores...")

    @log_operation("test_decorated_function", "main")
    def funcion_decorada(param1, param2="default"):
        """FunciÃ³n de prueba decorada"""
        logger = get_main_logger()
        logger.info(f"Ejecutando funciÃ³n con param1={param1}, param2={param2}")
        return len(param1) + len(param2)

    # FunciÃ³n exitosa
    resultado = funcion_decorada("test", "value")
    print(f"Resultado funciÃ³n decorada: {resultado}")

    # FunciÃ³n con error
    @log_operation("test_decorated_error", "main")
    def funcion_con_error():
        raise ValueError("Error en funciÃ³n decorada")

    try:
        funcion_con_error()
    except ValueError:
        pass  # Error esperado

    print("âœ… Decoradores completados")

def test_performance_tracking():
    """Prueba seguimiento de rendimiento"""
    print("ğŸ§ª Probando seguimiento de rendimiento...")

    logger = get_performance_logger()

    # Simular varias mÃ©tricas
    for i in range(5):
        logger.log_performance_metric("operation_time", 100 + i * 50, "ms")
        logger.log_performance_metric("memory_usage", 50 + i * 10, "MB")

    # Obtener resumen
    summary = logger.get_performance_summary()
    print("ğŸ“Š Resumen de rendimiento:")
    for metric, stats in summary.items():
        print(f"  {metric}: avg={stats['avg']:.2f}, min={stats['min']:.2f}, max={stats['max']:.2f}")

    print("âœ… Seguimiento de rendimiento completado")

def test_log_files():
    """Verifica que se generen archivos de log"""
    print("ğŸ§ª Verificando archivos de log...")

    # Obtener directorio de logs
    logger = get_main_logger()
    log_dir = logger.log_dir

    print(f"ğŸ“‚ Directorio de logs: {log_dir}")

    # Listar archivos generados
    if log_dir.exists():
        log_files = list(log_dir.glob("*.log")) + list(log_dir.glob("*.json"))
        print(f"ğŸ“„ Archivos de log encontrados: {len(log_files)}")

        for log_file in log_files:
            if log_file.exists():
                size = log_file.stat().st_size
                print(f"  ğŸ“‹ {log_file.name}: {size} bytes")

                # Verificar contenido de archivos JSON
                if log_file.suffix == '.json' and size > 0:
                    try:
                        with open(log_file, 'r', encoding='utf-8') as f:
                            first_line = f.readline().strip()
                            if first_line:
                                log_entry = json.loads(first_line)
                                print(f"    ğŸ“ Ejemplo de entrada: {log_entry.get('level', 'N/A')} - {log_entry.get('message', 'N/A')[:50]}...")
                    except Exception as e:
                        print(f"    âš ï¸ Error leyendo JSON: {e}")
    else:
        print("âš ï¸ Directorio de logs no existe aÃºn")

    print("âœ… VerificaciÃ³n de archivos completada")

def test_error_handling():
    """Prueba manejo avanzado de errores"""
    print("ğŸ§ª Probando manejo de errores...")

    # Usar context manager para captura de errores
    with log_errors("test_error_context", "main") as logger:
        logger.info("Dentro del contexto de manejo de errores")
        # Esta lÃ­nea causarÃ¡ un error
        raise ConnectionError("Error de conexiÃ³n simulado")

    print("âœ… Manejo de errores completado")

def main():
    """FunciÃ³n principal de pruebas"""
    print("ğŸš€ Iniciando pruebas del sistema de logging mejorado")
    print("=" * 60)

    try:
        test_basic_logging()
        print()

        test_operation_context()
        print()

        test_specialized_loggers()
        print()

        test_decorators()
        print()

        test_performance_tracking()
        print()

        test_log_files()
        print()

        # Esta prueba debe ir al final porque puede terminar con error
        try:
            test_error_handling()
        except ConnectionError:
            print("âœ… Manejo de errores completado (error esperado capturado)")
        print()

        print("=" * 60)
        print("ğŸ‰ Todas las pruebas de logging completadas exitosamente!")

        # Generar reporte final de rendimiento
        perf_logger = get_performance_logger()
        summary = perf_logger.get_performance_summary()

        if summary:
            print("\nğŸ“Š REPORTE FINAL DE RENDIMIENTO:")
            for metric, stats in summary.items():
                print(f"  ğŸ“ˆ {metric}:")
                print(f"    â€¢ Promedio: {stats['avg']:.2f}")
                print(f"    â€¢ MÃ­nimo: {stats['min']:.2f}")
                print(f"    â€¢ MÃ¡ximo: {stats['max']:.2f}")
                print(f"    â€¢ Total: {stats['total']:.2f}")
                print(f"    â€¢ Conteo: {stats['count']}")

    except Exception as e:
        print(f"âŒ Error durante las pruebas: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()